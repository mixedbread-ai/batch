{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:15:48.278414Z",
     "start_time": "2024-08-20T23:15:45.675852Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from batch.asyncio_.batch_processor import dynamically as async_dynamically\n",
    "from batch.thread_.batch_processor import dynamically as thread_dynamically\n",
    "\n",
    "from batch.inference.asyncio_ import dynamically as async_inference_dynamically\n",
    "from batch.inference.thread_ import dynamically as thread_inference_dynamically\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c248243d81d7cbd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:15:50.958798Z",
     "start_time": "2024-08-20T23:15:48.279331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load your model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b42d4caeb48a1b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:15:55.971936Z",
     "start_time": "2024-08-20T23:15:55.968460Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "encode = partial(\n",
    "    model.encode, \n",
    "    prompt_name=None, \n",
    "    prompt=None, \n",
    "    batch_size=32, \n",
    "    show_progress_bar=False,\n",
    "    output_value='sentence_embedding',\n",
    "    precision=\"float32\",\n",
    "    convert_to_tensor=False,\n",
    "    convert_to_numpy=True,\n",
    "    device=\"mps\",\n",
    "    normalize_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dcf854d97d95de",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:17:24.119993Z",
     "start_time": "2024-08-20T23:15:56.882123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time: 29.0773 seconds\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import timeit\n",
    "\n",
    "def benchmark():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "# Run the benchmark\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark, number=number_of_runs)\n",
    "\n",
    "# Calculate and print the average execution time\n",
    "average_time = execution_time / number_of_runs\n",
    "print(f\"Average execution time: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01ab6dc24fa5a9f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:17:32.338544Z",
     "start_time": "2024-08-20T23:17:26.894539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time for async_encode: 1.8133 seconds\n"
     ]
    }
   ],
   "source": [
    "async_encode = async_dynamically(encode)\n",
    "\n",
    "def benchmark_async():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(async_encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "# Run the benchmark\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark_async, number=number_of_runs)\n",
    "\n",
    "# Calculate and print the average execution time\n",
    "average_time = execution_time / number_of_runs\n",
    "print(f\"Average execution time for async_encode: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e050b464787a1209",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:17:39.697111Z",
     "start_time": "2024-08-20T23:17:34.416519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time for thread_encode: 1.7592 seconds\n"
     ]
    }
   ],
   "source": [
    "thread_encode = thread_dynamically(encode)\n",
    "\n",
    "def benchmark_thread():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(thread_encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "# Run the benchmark\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark_thread, number=number_of_runs)\n",
    "\n",
    "# Calculate and print the average execution time\n",
    "average_time = execution_time / number_of_runs\n",
    "print(f\"Average execution time for thread_encode: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b519d67d7c8a37",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:17:45.575540Z",
     "start_time": "2024-08-20T23:17:42.912644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/ofen/models/torch/text_encoder.py:144: UserWarning: Pooling strategy is set to NONE. This may result in unexpected behavior as the output will be the full sequence of hidden states rather than a single vector per input. Consider using a different pooling strategy such as mean, max, or cls if you need a fixed-size representation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ofen.models import TextEncoder\n",
    "model = TextEncoder(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "def ofen_encode(texts):\n",
    "    results = model.encode(\n",
    "        texts, \n",
    "        batch_size=32, \n",
    "        show_progress=False, \n",
    "        normalize=True, \n",
    "        dimensions=None, \n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return results.embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c87cf5eee759e5e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:18:59.208420Z",
     "start_time": "2024-08-20T23:17:46.478896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 24.2413 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def benchmark():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(ofen_encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark, number=number_of_runs)\n",
    "average_time = execution_time / number_of_runs\n",
    "\n",
    "print(f\"Execution time: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e3501351c771c0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:19:07.127685Z",
     "start_time": "2024-08-20T23:19:01.925819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async OFen Execution time: 1.7326 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "async_ofen_encode = async_dynamically(ofen_encode)\n",
    "\n",
    "def benchmark_async_ofen():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(async_ofen_encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark_async_ofen, number=number_of_runs)\n",
    "average_time = execution_time / number_of_runs\n",
    "\n",
    "print(f\"Async OFen Execution time: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b5a2e66cd631f3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:19:16.028096Z",
     "start_time": "2024-08-20T23:19:10.919560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread OFen Execution time: 1.7014 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "thread_ofen_encode = thread_dynamically(ofen_encode)\n",
    "\n",
    "def benchmark_thread_ofen():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(thread_ofen_encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark_thread_ofen, number=number_of_runs)\n",
    "average_time = execution_time / number_of_runs\n",
    "print(f\"Thread OFen Execution time: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e97955a6dcd289",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:19:22.514363Z",
     "start_time": "2024-08-20T23:19:22.512013Z"
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "forward = model.forward\n",
    "\n",
    "def the_forward(features: dict):\n",
    "    return forward(**features)[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a28487edc7a3f224",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:19:29.529003Z",
     "start_time": "2024-08-20T23:19:24.119449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async Inference Execution time: 1.8018 seconds\n"
     ]
    }
   ],
   "source": [
    "batched_forward = async_inference_dynamically(the_forward)\n",
    "model.forward = lambda **kwargs: {\"embeddings\": batched_forward(**kwargs)}\n",
    "\n",
    "def benchmark():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(model.encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark, number=number_of_runs)\n",
    "average_time = execution_time / number_of_runs\n",
    "print(f\"Async Inference Execution time: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d43f601601739e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T23:19:39.297958Z",
     "start_time": "2024-08-20T23:19:33.899506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread Inference Execution time: 1.7984 seconds\n"
     ]
    }
   ],
   "source": [
    "batched_forward = thread_inference_dynamically(the_forward)\n",
    "model.forward = lambda **kwargs: {\"embeddings\": batched_forward(**kwargs)}\n",
    "\n",
    "def benchmark_thread_inference():\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        futures = [executor.submit(model.encode, [\"Hello, world!\"]) for _ in range(1000)]\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "number_of_runs = 3\n",
    "execution_time = timeit.timeit(benchmark_thread_inference, number=number_of_runs)\n",
    "average_time = execution_time / number_of_runs\n",
    "print(f\"Thread Inference Execution time: {average_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c29bde9139b4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
