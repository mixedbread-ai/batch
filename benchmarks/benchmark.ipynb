{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from batch import BatchProcessor\n",
    "from batch.asyncio.batch_processor import BatchProcessor\n",
    "from batch.types import ModelFeatures\n",
    "\n",
    "\n",
    "def mock_inference_func(features: ModelFeatures) -> ModelOutputs:\n",
    "    sleep(0.001)\n",
    "    return {\n",
    "        \"output\": np.random.rand(100, 100),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "model.encode(texts=[\"text a\", \"text b\"], ids=[1, 2], normalize_embeddings=True)\n",
    "\n",
    "model.forward = batch_dynamically(batch_size=4, max_latency=0.1)(model.forwad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.encode([\"text a\", \"text b\"])\n",
    "model.encode([\"text a\", \"text b\"])\n",
    "\n",
    "@batch_dynamically(batch_size=4, max_latency=0.1)\n",
    "def create_embeddings(req: List[ModelFeatures]) -> List[ModelOutputs]:\n",
    "    processor = BatchProcessor(\n",
    "        model_fn=mock_inference_func,\n",
    "        batch_size=4,\n",
    "        max_latency=0.1,\n",
    "    )\n",
    "    return processor.process(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings([ModelFeatures(texts=[\"text a\", \"text b\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
